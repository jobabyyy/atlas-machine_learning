{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gef2LjwnjtY",
        "outputId": "49c0e803-73de-4185-9168-5f2312d073ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trJagFkfS62S",
        "outputId": "c050ea95-61f1-453a-d78a-a58a4480ac5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.1  1.1  1.   1.   1.   1.   1.   1. ]\n",
            " [ 1.1  1.1  1.   1.   1.   1.   1.   1. ]\n",
            " [ 1.1  1.1  1.1 -1.   1.   1.   1.   1. ]\n",
            " [ 1.   1.   1.   1.   1.  -1.   1.   1. ]\n",
            " [ 1.   1.   1.  -1.   1.   1.   1.   1. ]\n",
            " [ 1.  -1.  -1.   1.   1.   1.  -1.   1. ]\n",
            " [ 1.  -1.   1.   1.  -1.   1.  -1.   1. ]\n",
            " [ 1.   1.   1.  -1.   1.   1.   1.   1. ]]\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\" Temporal Difference:\n",
        "    Task 0 - Monte Carlo Method\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def monte_carlo(env, V, policy, episodes=5000,\n",
        "                max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    \"\"\"\n",
        "    function that performs the Monte Carlo Algorithm\n",
        "    - env: is the openAI environment instance\n",
        "    - V: is a numpy.ndarray of shape (s,)\n",
        "         containing the value estimate\n",
        "    - policy: is a function that takes in a state\n",
        "              and returns the next action to take\n",
        "    - episodes: is the total number of episodes\n",
        "                to train over\n",
        "    - max_steps: is the maximum number of steps\n",
        "                 per episode\n",
        "    - alpha: is the learning rate\n",
        "    - gamma: is the discount rate\n",
        "    Returns: V, the updated value estimate\n",
        "    \"\"\"\n",
        "    for episode in range(episodes):\n",
        "      # init empty list to store trajectory\n",
        "      trajectory = []\n",
        "\n",
        "      # reset env for new episode\n",
        "      state = env.reset()\n",
        "\n",
        "      for step in range(max_steps):\n",
        "        # action chosen; using policy func.\n",
        "        action = policy(state)\n",
        "        # observing chosen action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        trajectory.append((state, action, reward))\n",
        "        # update the current state\n",
        "        state = next_state\n",
        "\n",
        "        # break out of loop if episode is done.\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    # update value using monte carlo method.\n",
        "    # init return\n",
        "    G = 0\n",
        "    # recording value of state at first visit\n",
        "    first_first_val = set()\n",
        "\n",
        "    for state, action, reward in reversed(trajectory):\n",
        "      # update reward using discount factor.\n",
        "      G = gamma * G + reward\n",
        "      if state not in first_first_val:\n",
        "        first_first_val.add(state)\n",
        "        V[state] = V[state] + alpha * (G + V[state])\n",
        "\n",
        "    return V\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "  import gym\n",
        "  import numpy as np\n",
        "\n",
        "  np.random.seed(0)\n",
        "\n",
        "  env = gym.make('FrozenLake8x8-v1')\n",
        "  LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "  def policy(s):\n",
        "      p = np.random.uniform()\n",
        "      if p > 0.5:\n",
        "          if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "              return RIGHT\n",
        "          elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "              return DOWN\n",
        "          elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "              return UP\n",
        "          else:\n",
        "              return LEFT\n",
        "      else:\n",
        "          if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "              return DOWN\n",
        "          elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "              return RIGHT\n",
        "          elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "              return LEFT\n",
        "          else:\n",
        "              return UP\n",
        "\n",
        "  V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "  np.set_printoptions(precision=2)\n",
        "  env.seed(0)\n",
        "  print(monte_carlo(env, V, policy).reshape((8, 8)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Temporal Difference:\n",
        "Task 1 - TD(位).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def td_lambtha(env, V, policy, lambtha, episodes=5000,\n",
        "               max_steps=100, alpha=0.1, gamma=0.99):\n",
        "  \"\"\"\n",
        "  function that performs the TD(位) algorithm:\n",
        "\n",
        "  env: is the openAI environment instance\n",
        "  V: is a numpy.ndarray of shape (s,) containing\n",
        "     the value estimate\n",
        "  policy: is a function that takes in a state\n",
        "          and returns the next action to take\n",
        "  lambtha: is the eligibility trace factor\n",
        "  episodes: is the total number of episodes\n",
        "            to train over\n",
        "  max_steps: is the maximum number of steps\n",
        "             per episode\n",
        "  alpha: is the learning rate\n",
        "  gamma: is the discount rate\n",
        "\n",
        "  Returns: V, the updated value estimate\n",
        "  \"\"\"\n",
        "\n",
        "  for _ in range(episodes):\n",
        "    # updating the current state\n",
        "    state = env.reset()\n",
        "    # array created to track eligibility\n",
        "    eligibility = np.zeros_like(V)\n",
        "\n",
        "    \"\"\"\n",
        "    Loop allows agent to interact w/env and\n",
        "    selects actions based on policy, collects\n",
        "    experiences and updates its value estimates\n",
        "    \"\"\"\n",
        "    for _ in range(max_steps):\n",
        "      action = policy(state)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      # computing the TD error\n",
        "      delta = reward + gamma * V[next_state] - V[state]\n",
        "\n",
        "      # updating eligibility trace 4 current state\n",
        "      eligibility[state] += 1\n",
        "\n",
        "      \"\"\"\n",
        "      Loop updates the value estimates for all states\n",
        "      in env based on TD error & updates the eligibity\n",
        "      traces for each state.\n",
        "      \"\"\"\n",
        "      for s in range(V.shape[0]):\n",
        "        V[s] += alpha * delta * eligibility[s]\n",
        "        eligibility[s] *= lambtha * gamma\n",
        "\n",
        "      if done:\n",
        "          break\n",
        "\n",
        "      # updating state\n",
        "      state = next_state\n",
        "\n",
        "  return V\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  import gym\n",
        "  import numpy as np\n",
        "\n",
        "  np.random.seed(0)\n",
        "\n",
        "  env = gym.make('FrozenLake8x8-v1')\n",
        "  LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "  def policy(s):\n",
        "      p = np.random.uniform()\n",
        "      if p > 0.5:\n",
        "          if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "              return RIGHT\n",
        "          elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "              return DOWN\n",
        "          elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "              return UP\n",
        "          else:\n",
        "              return LEFT\n",
        "      else:\n",
        "          if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "              return DOWN\n",
        "          elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "              return RIGHT\n",
        "          elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "              return LEFT\n",
        "          else:\n",
        "              return UP\n",
        "\n",
        "  V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "  np.set_printoptions(precision=4)\n",
        "  print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvseDqqVXHKn",
        "outputId": "ea649d42-1ba7-4af9-855d-eea340f1d6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.8501 -0.8302 -0.8495 -0.7838 -0.7288 -0.7237 -0.6867 -0.6895]\n",
            " [-0.8824 -0.8829 -0.8819 -0.8526 -0.8205 -0.7441 -0.68   -0.8189]\n",
            " [-0.9108 -0.9199 -0.9537 -1.     -0.869  -0.7854 -0.6057 -0.7755]\n",
            " [-0.9326 -0.9399 -0.9507 -0.9431 -0.8995 -1.     -0.6609 -0.6571]\n",
            " [-0.9531 -0.9605 -0.9548 -1.     -0.929  -0.8948 -0.7488 -0.4105]\n",
            " [-0.9531 -1.     -1.      0.5122 -0.9688 -0.9453 -1.     -0.1842]\n",
            " [-0.9395 -1.     -0.5721 -0.0116 -1.     -0.5796 -1.      0.4346]\n",
            " [-0.9313 -0.9581 -0.9247 -1.      1.      1.1641  1.099   1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Temporal Difference:\n",
        "Task 2 - SARSA(位)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def epsilon_greedy_policy(Q, state, epsilon):\n",
        "  if random.uniform(0, 1) < epsilon:\n",
        "    return random.randint(0, Q.shape[1] -1)\n",
        "  else:\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100,\n",
        "                  alpha=0.1, gamma=0.99, epsilon=1,\n",
        "                  min_epsilon=0.1, epsilon_decay=0.05):\n",
        "  \"\"\"\n",
        "  function that performs SARSA(位):\n",
        "\n",
        "  env: is the openAI environment instance\n",
        "  Q: is a numpy.ndarray of shape (s,a)\n",
        "     containing the Q table\n",
        "  lambtha: is the eligibility trace factor\n",
        "  episodes: is the total number of episodes\n",
        "            to train over\n",
        "  max_steps: is the maximum number of\n",
        "             steps per episode\n",
        "  alpha: is the learning rate\n",
        "  gamma: is the discount rate\n",
        "  epsilon: is the initial threshold for\n",
        "           epsilon greedy\n",
        "  min_epsilon: is the minimum value that epsilon\n",
        "               should decay to\n",
        "  epsilon_decay: is the decay rate for updating\n",
        "                 epsilon between episodes\n",
        "\n",
        "  Returns: Q, the updated Q table\n",
        "  \"\"\"\n",
        "\n",
        "  n_states, n_actions = Q.shape\n",
        "\n",
        "  for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    eligibility = np.zeros_like(Q)\n",
        "    action = epsilon_greedy_policy(Q, state, epsilon)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      next_action = epsilon_greedy_policy(Q, next_state, epsilon)\n",
        "\n",
        "      # update SARSA\n",
        "      delta = reward + gamma * Q[next_state, next_action] - Q[state, action]\n",
        "      eligibility[state, action] += 1\n",
        "\n",
        "      # update qvalue using the SARSA update rule.\n",
        "      for s in range(n_states):\n",
        "        for a in range(n_actions):\n",
        "          Q[s, a] += alpha * delta * eligibility[s, a]\n",
        "          eligibility[s, a] *= lambtha * gamma\n",
        "\n",
        "      if done:\n",
        "         break\n",
        "\n",
        "      state = next_state\n",
        "      action = next_action\n",
        "\n",
        "    epsilon = max(min_epsilon, epsilon - epsilon_decay)\n",
        "\n",
        "  return Q\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  import gym\n",
        "  import numpy as np\n",
        "\n",
        "  np.random.seed(0)\n",
        "  env = gym.make('FrozenLake8x8-v1')\n",
        "  Q = np.random.uniform(size=(64, 4))\n",
        "  np.set_printoptions(precision=4)\n",
        "  print(sarsa_lambtha(env, Q, 0.9))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5b9qwMbiWOT",
        "outputId": "fc0a869d-a6cf-4f5b-c261-91f1f3e187a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5851 0.6301 0.6096 0.6064]\n",
            " [0.6005 0.6069 0.5944 0.6324]\n",
            " [0.6729 0.6276 0.6039 0.598 ]\n",
            " [0.5817 0.6267 0.6156 0.6374]\n",
            " [0.673  0.6743 0.6713 0.6608]\n",
            " [0.6722 0.6911 0.6728 0.6642]\n",
            " [0.6593 0.7258 0.6783 0.6586]\n",
            " [0.7369 0.6752 0.6429 0.6541]\n",
            " [0.6256 0.6412 0.633  0.5487]\n",
            " [0.5862 0.6121 0.6214 0.6105]\n",
            " [0.6115 0.6648 0.6241 0.6222]\n",
            " [0.5387 0.5763 0.5637 0.6578]\n",
            " [0.6589 0.6787 0.7121 0.6602]\n",
            " [0.7133 0.7593 0.7051 0.7266]\n",
            " [0.7284 0.6827 0.735  0.6008]\n",
            " [0.5654 0.6283 0.7634 0.3472]\n",
            " [0.6695 0.6877 0.6689 0.6614]\n",
            " [0.6554 0.7322 0.657  0.6657]\n",
            " [0.6874 0.6054 0.5139 0.5311]\n",
            " [0.2828 0.1202 0.2961 0.1187]\n",
            " [0.5902 0.595  0.769  0.5407]\n",
            " [0.7737 0.7474 0.77   0.749 ]\n",
            " [0.6951 0.7502 0.6833 0.7614]\n",
            " [0.6622 0.7203 0.6135 0.7308]\n",
            " [0.7012 0.6844 0.6836 0.7395]\n",
            " [0.7279 0.7224 0.8011 0.7274]\n",
            " [0.7014 0.7879 0.71   0.6458]\n",
            " [0.5208 0.8016 0.5745 0.5892]\n",
            " [0.7373 0.747  0.7401 0.7725]\n",
            " [0.8811 0.5813 0.8817 0.6925]\n",
            " [0.8327 0.7426 0.7704 0.7373]\n",
            " [0.6794 0.839  0.6408 0.5798]\n",
            " [0.764  0.7038 0.712  0.628 ]\n",
            " [0.7052 0.8226 0.7879 0.7636]\n",
            " [0.7148 0.7405 0.7176 0.8049]\n",
            " [0.8965 0.3676 0.4359 0.8919]\n",
            " [0.8354 0.7736 0.3474 0.8139]\n",
            " [0.7824 0.8795 0.2891 0.8002]\n",
            " [0.3375 0.7665 0.6167 0.8735]\n",
            " [0.8959 0.7914 0.8498 0.5087]\n",
            " [0.6376 0.5736 0.8208 0.6418]\n",
            " [0.9755 0.8558 0.0117 0.36  ]\n",
            " [0.73   0.1716 0.521  0.0543]\n",
            " [0.2523 0.0683 0.8681 0.2666]\n",
            " [0.4862 0.7866 0.9161 0.4493]\n",
            " [0.3644 0.9025 0.5959 0.5126]\n",
            " [0.9342 0.614  0.5356 0.5899]\n",
            " [1.0259 0.4572 0.5146 0.4016]\n",
            " [0.3753 0.4345 0.6257 0.4277]\n",
            " [0.2274 0.2544 0.058  0.4344]\n",
            " [0.3118 0.6287 0.3778 0.1796]\n",
            " [0.0247 0.0672 0.8165 0.454 ]\n",
            " [0.5366 0.8967 0.9903 0.2169]\n",
            " [0.6631 0.4419 0.1833 0.8679]\n",
            " [0.32   0.3835 0.5883 0.831 ]\n",
            " [0.757  1.1926 0.2735 0.9017]\n",
            " [0.2414 0.4489 0.5103 0.3112]\n",
            " [0.4564 0.4466 0.3202 0.2545]\n",
            " [0.4654 0.0812 0.2075 0.4247]\n",
            " [0.3742 0.4636 0.2776 0.5868]\n",
            " [0.8755 0.1175 0.5174 0.1321]\n",
            " [0.7411 0.3961 0.5654 0.1833]\n",
            " [0.1448 0.4881 0.3556 0.9404]\n",
            " [0.7653 0.7487 0.9037 0.0834]]\n"
          ]
        }
      ]
    }
  ]
}
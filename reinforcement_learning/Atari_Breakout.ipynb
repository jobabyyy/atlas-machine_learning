{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPClo4B9Whbv6gi9F8HwWeb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobabyyy/atlas-machine_learning/blob/main/Atari_Breakout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LJduk-2AUs0"
      },
      "outputs": [],
      "source": [
        "!pip install keras-rl2\n",
        "!pip install gym[atari]\n",
        "!pip install atari-py\n",
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "c1YBu7EfCD5T"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYpQvRWAkKw",
        "outputId": "b5eb3e8a-849e-4731-97e8-b8cf2b7930c5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m atari_py.import_roms /content/drive/MyDrive/dqn/roms/"
      ],
      "metadata": {
        "id": "-vRVFQkOAp0n"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.envs.registration import register\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "metadata": {
        "id": "12H3ViSwAuVN"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "register(\n",
        "    id='Breakout-v4',\n",
        "    entry_point='gym.envs.atari:AtariEnv',\n",
        "    kwargs={'game': 'breakout', 'obs_type': 'image', 'frameskip': 1},\n",
        "    max_episode_steps=10000,\n",
        "    nondeterministic=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6PT9H0jMREy",
        "outputId": "16017a63-547c-4327-98bc-686276fc43a6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment Breakout-v4\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[accept-rom-license]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD5zL32vPttA",
        "outputId": "9108fb1c-7d4e-41ca-dabe-51ad922529b0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gym\n",
        "!pip install gym[atari,accept-rom-license]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "pslCVTOoQLbC",
        "outputId": "792ed1db-34ac-4460-9496-b6968400d842"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/gym/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.10/dist-packages/gym/envs/atari/__init__.py\n",
            "    /usr/local/lib/python3.10/dist-packages/gym/envs/atari/environment.py\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "Collecting gym[accept-rom-license,atari]\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Collecting ale-py~=0.8.0 (from gym[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (6.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2023.11.17)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827624 sha256=b7e29177ecc7dd3a9aa83cbae8f487ddb44840cfbc9eeb2ebe291583bdbe3935\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym, ale-py\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.7.5\n",
            "    Uninstalling ale-py-0.7.5:\n",
            "      Successfully uninstalled ale-py-0.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.8.1 gym-0.26.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ale_py",
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gym\n",
        "!pip install gym==0.25.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPlKTE1pQequ",
        "outputId": "b2d32768-fba4-49fb-9930-3c1f4dbe7f6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/gym/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "Collecting gym==0.25.2\n",
            "  Using cached gym-0.25.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Installing collected packages: gym\n",
            "Successfully installed gym-0.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from gym.wrappers import AtariPreprocessing\n",
        "\n",
        "class CompatibleAtariPreprocessing(gym.Wrapper):\n",
        "    def __init__(self, env, **kwargs):\n",
        "        super().__init__(env)\n",
        "        self.env = AtariPreprocessing(env, **kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Handling the returned values correctly\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        # Add a reset method to handle environment reset\n",
        "        return self.env.reset()\n",
        "\n",
        "# Custom callback for DQNAgent\n",
        "class DQNAgentCheckpoint(Callback):\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.model.save_weights(self.filepath.format(epoch=epoch), overwrite=True)\n",
        "\n",
        "# Environment setup with custom preprocessing wrapper\n",
        "env = gym.make('Breakout-v4')\n",
        "env = CompatibleAtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=1, scale_obs=True)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Adjusting the model to the preprocessed observation space\n",
        "input_shape = (1,) + env.observation_space.shape\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "model.add(Dense(24, activation='relu'))\n",
        "model.add(Dense(24, activation='relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "\n",
        "# Configure and compile the agent\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = EpsGreedyQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "# Custom checkpoint to save model after every epoch\n",
        "checkpoint = DQNAgentCheckpoint('policy_{epoch}.h5')\n",
        "\n",
        "# Training\n",
        "dqn.fit(env, nb_steps=80000, visualize=False, verbose=2, callbacks=[checkpoint])\n",
        "\n",
        "# Save the final policy network after training\n",
        "dqn.save_weights('policy_final.h5', overwrite=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reJ4Gx8XA2a5",
        "outputId": "2952e380-8b11-4f3a-c126-99e520a59ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 80000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'tuple'>\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:226: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
            "  logger.warn(\"Casting input x to numpy array.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:167: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space with exception: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space with exception: {e}\")\n",
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   239/80000: episode: 1, duration: 13.749s, episode steps: 239, steps per second:  17, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.682 [0.000, 3.000],  loss: 0.001754, mae: 0.193631, mean_q: 0.028738\n",
            "   606/80000: episode: 2, duration: 15.481s, episode steps: 367, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.001400, mae: 0.010382, mean_q: 0.015752\n",
            "  1030/80000: episode: 3, duration: 17.436s, episode steps: 424, steps per second:  24, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.002717, mae: 0.032121, mean_q: 0.046273\n",
            "  1400/80000: episode: 4, duration: 16.888s, episode steps: 370, steps per second:  22, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.216 [0.000, 3.000],  loss: 0.002737, mae: 0.050990, mean_q: 0.073396\n",
            "  1660/80000: episode: 5, duration: 10.923s, episode steps: 260, steps per second:  24, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.227 [0.000, 3.000],  loss: 0.003893, mae: 0.074256, mean_q: 0.103868\n",
            "  1956/80000: episode: 6, duration: 12.900s, episode steps: 296, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003176, mae: 0.086047, mean_q: 0.118184\n",
            "  2276/80000: episode: 7, duration: 13.667s, episode steps: 320, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.003288, mae: 0.100112, mean_q: 0.137731\n",
            "  2575/80000: episode: 8, duration: 13.083s, episode steps: 299, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.003822, mae: 0.115521, mean_q: 0.159334\n",
            "  2917/80000: episode: 9, duration: 14.646s, episode steps: 342, steps per second:  23, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.004810, mae: 0.134958, mean_q: 0.186040\n",
            "  3199/80000: episode: 10, duration: 12.836s, episode steps: 282, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.003276, mae: 0.148343, mean_q: 0.201041\n",
            "  3505/80000: episode: 11, duration: 13.243s, episode steps: 306, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.003849, mae: 0.160333, mean_q: 0.219633\n",
            "  3717/80000: episode: 12, duration: 8.229s, episode steps: 212, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.003739, mae: 0.175054, mean_q: 0.237523\n",
            "  4006/80000: episode: 13, duration: 12.759s, episode steps: 289, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.003109, mae: 0.180882, mean_q: 0.245818\n",
            "  4447/80000: episode: 14, duration: 18.113s, episode steps: 441, steps per second:  24, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.003237, mae: 0.193421, mean_q: 0.261269\n",
            "  4742/80000: episode: 15, duration: 12.777s, episode steps: 295, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.003465, mae: 0.204049, mean_q: 0.276337\n",
            "  5013/80000: episode: 16, duration: 12.021s, episode steps: 271, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: 0.003373, mae: 0.213374, mean_q: 0.288657\n",
            "  5402/80000: episode: 17, duration: 16.911s, episode steps: 389, steps per second:  23, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.003243, mae: 0.223810, mean_q: 0.301691\n",
            "  5721/80000: episode: 18, duration: 14.305s, episode steps: 319, steps per second:  22, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.004210, mae: 0.235296, mean_q: 0.316739\n",
            "  5997/80000: episode: 19, duration: 12.702s, episode steps: 276, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.004315, mae: 0.246970, mean_q: 0.333655\n",
            "  6351/80000: episode: 20, duration: 15.230s, episode steps: 354, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.003512, mae: 0.255560, mean_q: 0.345169\n",
            "  6745/80000: episode: 21, duration: 16.965s, episode steps: 394, steps per second:  23, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.003786, mae: 0.267926, mean_q: 0.361406\n",
            "  7024/80000: episode: 22, duration: 12.520s, episode steps: 279, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.003370, mae: 0.275863, mean_q: 0.370912\n",
            "  7344/80000: episode: 23, duration: 13.981s, episode steps: 320, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.004250, mae: 0.282960, mean_q: 0.382052\n",
            "  7633/80000: episode: 24, duration: 12.564s, episode steps: 289, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.003284, mae: 0.290869, mean_q: 0.391060\n",
            "  7923/80000: episode: 25, duration: 12.889s, episode steps: 290, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.002814, mae: 0.294630, mean_q: 0.396746\n",
            "  8341/80000: episode: 26, duration: 17.435s, episode steps: 418, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 0.003588, mae: 0.300385, mean_q: 0.405473\n",
            "  8728/80000: episode: 27, duration: 16.352s, episode steps: 387, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.003405, mae: 0.311926, mean_q: 0.418938\n",
            "  9120/80000: episode: 28, duration: 16.344s, episode steps: 392, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.004134, mae: 0.321205, mean_q: 0.432351\n",
            "  9453/80000: episode: 29, duration: 14.522s, episode steps: 333, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003055, mae: 0.327557, mean_q: 0.439973\n",
            "  9725/80000: episode: 30, duration: 12.139s, episode steps: 272, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002694, mae: 0.327436, mean_q: 0.438976\n",
            " 10171/80000: episode: 31, duration: 18.527s, episode steps: 446, steps per second:  24, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.274 [0.000, 3.000],  loss: 0.003438, mae: 0.334752, mean_q: 0.449683\n",
            " 10500/80000: episode: 32, duration: 14.389s, episode steps: 329, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.252 [0.000, 3.000],  loss: 0.003563, mae: 0.341592, mean_q: 0.459816\n",
            " 10824/80000: episode: 33, duration: 14.168s, episode steps: 324, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.198 [0.000, 3.000],  loss: 0.003807, mae: 0.345308, mean_q: 0.462220\n",
            " 11190/80000: episode: 34, duration: 15.458s, episode steps: 366, steps per second:  24, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.003457, mae: 0.349693, mean_q: 0.468962\n",
            " 11557/80000: episode: 35, duration: 16.123s, episode steps: 367, steps per second:  23, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.240 [0.000, 3.000],  loss: 0.004201, mae: 0.360113, mean_q: 0.483168\n",
            " 11770/80000: episode: 36, duration: 9.667s, episode steps: 213, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.258 [0.000, 3.000],  loss: 0.004239, mae: 0.365992, mean_q: 0.491051\n",
            " 12031/80000: episode: 37, duration: 11.747s, episode steps: 261, steps per second:  22, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002649, mae: 0.365011, mean_q: 0.489598\n",
            " 12398/80000: episode: 38, duration: 15.715s, episode steps: 367, steps per second:  23, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.913 [0.000, 3.000],  loss: 0.003343, mae: 0.367669, mean_q: 0.492294\n",
            " 12647/80000: episode: 39, duration: 11.297s, episode steps: 249, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.003600, mae: 0.370778, mean_q: 0.499159\n",
            " 13010/80000: episode: 40, duration: 16.221s, episode steps: 363, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.292 [0.000, 3.000],  loss: 0.003737, mae: 0.378096, mean_q: 0.506263\n",
            " 13227/80000: episode: 41, duration: 8.487s, episode steps: 217, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.931 [0.000, 3.000],  loss: 0.003583, mae: 0.381947, mean_q: 0.509722\n",
            " 13567/80000: episode: 42, duration: 14.563s, episode steps: 340, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002983, mae: 0.381678, mean_q: 0.511334\n",
            " 13795/80000: episode: 43, duration: 10.426s, episode steps: 228, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.851 [0.000, 3.000],  loss: 0.003663, mae: 0.387290, mean_q: 0.519375\n",
            " 14094/80000: episode: 44, duration: 13.026s, episode steps: 299, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.027 [0.000, 3.000],  loss: 0.003075, mae: 0.386211, mean_q: 0.518479\n",
            " 14561/80000: episode: 45, duration: 19.699s, episode steps: 467, steps per second:  24, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.003320, mae: 0.389743, mean_q: 0.522537\n",
            " 14886/80000: episode: 46, duration: 14.176s, episode steps: 325, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.003297, mae: 0.395069, mean_q: 0.529113\n",
            " 15078/80000: episode: 47, duration: 9.299s, episode steps: 192, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003586, mae: 0.398067, mean_q: 0.531916\n",
            " 15489/80000: episode: 48, duration: 17.371s, episode steps: 411, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.182 [0.000, 3.000],  loss: 0.003578, mae: 0.397978, mean_q: 0.533403\n",
            " 15871/80000: episode: 49, duration: 16.181s, episode steps: 382, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.003636, mae: 0.399864, mean_q: 0.535398\n",
            " 16204/80000: episode: 50, duration: 14.346s, episode steps: 333, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003150, mae: 0.403681, mean_q: 0.539484\n",
            " 16608/80000: episode: 51, duration: 17.267s, episode steps: 404, steps per second:  23, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.003456, mae: 0.405788, mean_q: 0.543377\n",
            " 16828/80000: episode: 52, duration: 10.048s, episode steps: 220, steps per second:  22, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.209 [0.000, 3.000],  loss: 0.003342, mae: 0.408261, mean_q: 0.547060\n",
            " 17129/80000: episode: 53, duration: 13.271s, episode steps: 301, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.003887, mae: 0.412824, mean_q: 0.551732\n",
            " 17435/80000: episode: 54, duration: 13.344s, episode steps: 306, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.003864, mae: 0.414488, mean_q: 0.556748\n",
            " 17906/80000: episode: 55, duration: 19.618s, episode steps: 471, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.003109, mae: 0.417176, mean_q: 0.559108\n",
            " 18080/80000: episode: 56, duration: 8.607s, episode steps: 174, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.003712, mae: 0.421531, mean_q: 0.565489\n",
            " 18408/80000: episode: 57, duration: 14.263s, episode steps: 328, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.002628, mae: 0.416108, mean_q: 0.556489\n",
            " 18723/80000: episode: 58, duration: 13.924s, episode steps: 315, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.004006, mae: 0.418025, mean_q: 0.559589\n",
            " 19075/80000: episode: 59, duration: 15.091s, episode steps: 352, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.003311, mae: 0.421814, mean_q: 0.564672\n",
            " 19403/80000: episode: 60, duration: 14.286s, episode steps: 328, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 0.003712, mae: 0.423448, mean_q: 0.567479\n",
            " 19733/80000: episode: 61, duration: 14.160s, episode steps: 330, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.685 [0.000, 3.000],  loss: 0.003653, mae: 0.424806, mean_q: 0.568792\n",
            " 19958/80000: episode: 62, duration: 8.507s, episode steps: 225, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.916 [0.000, 3.000],  loss: 0.004644, mae: 0.432769, mean_q: 0.577932\n",
            " 20415/80000: episode: 63, duration: 20.912s, episode steps: 457, steps per second:  22, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.003242, mae: 0.428479, mean_q: 0.573365\n",
            " 20661/80000: episode: 64, duration: 9.093s, episode steps: 246, steps per second:  27, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: 0.004392, mae: 0.433510, mean_q: 0.577824\n",
            " 20957/80000: episode: 65, duration: 13.032s, episode steps: 296, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.003464, mae: 0.434320, mean_q: 0.580277\n",
            " 21237/80000: episode: 66, duration: 12.457s, episode steps: 280, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.003029, mae: 0.431475, mean_q: 0.576582\n",
            " 21793/80000: episode: 67, duration: 24.627s, episode steps: 556, steps per second:  23, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.766 [0.000, 3.000],  loss: 0.003395, mae: 0.434506, mean_q: 0.581043\n",
            " 22053/80000: episode: 68, duration: 10.641s, episode steps: 260, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.185 [0.000, 3.000],  loss: 0.003923, mae: 0.435978, mean_q: 0.582652\n",
            " 22391/80000: episode: 69, duration: 14.456s, episode steps: 338, steps per second:  23, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.003409, mae: 0.436551, mean_q: 0.584395\n",
            " 22755/80000: episode: 70, duration: 16.563s, episode steps: 364, steps per second:  22, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.003910, mae: 0.437387, mean_q: 0.584817\n",
            " 23062/80000: episode: 71, duration: 12.678s, episode steps: 307, steps per second:  24, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.987 [0.000, 3.000],  loss: 0.003239, mae: 0.437325, mean_q: 0.584194\n",
            " 23351/80000: episode: 72, duration: 12.054s, episode steps: 289, steps per second:  24, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.246 [0.000, 3.000],  loss: 0.003338, mae: 0.437909, mean_q: 0.584482\n",
            " 23535/80000: episode: 73, duration: 8.973s, episode steps: 184, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.003062, mae: 0.435620, mean_q: 0.581798\n",
            " 23752/80000: episode: 74, duration: 9.333s, episode steps: 217, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.281 [0.000, 3.000],  loss: 0.003381, mae: 0.435988, mean_q: 0.583124\n",
            " 24114/80000: episode: 75, duration: 16.298s, episode steps: 362, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002911, mae: 0.430808, mean_q: 0.575488\n",
            " 24395/80000: episode: 76, duration: 11.123s, episode steps: 281, steps per second:  25, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.854 [0.000, 3.000],  loss: 0.003747, mae: 0.432632, mean_q: 0.580042\n",
            " 24810/80000: episode: 77, duration: 19.498s, episode steps: 415, steps per second:  21, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.003304, mae: 0.431826, mean_q: 0.577472\n",
            " 25005/80000: episode: 78, duration: 7.390s, episode steps: 195, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.003757, mae: 0.432556, mean_q: 0.579249\n",
            " 25347/80000: episode: 79, duration: 14.979s, episode steps: 342, steps per second:  23, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.161 [0.000, 3.000],  loss: 0.003353, mae: 0.430395, mean_q: 0.576273\n",
            " 25545/80000: episode: 80, duration: 9.733s, episode steps: 198, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.003348, mae: 0.428219, mean_q: 0.573952\n",
            " 25843/80000: episode: 81, duration: 13.263s, episode steps: 298, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.003565, mae: 0.430097, mean_q: 0.573422\n",
            " 26083/80000: episode: 82, duration: 10.590s, episode steps: 240, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.929 [0.000, 3.000],  loss: 0.003386, mae: 0.428450, mean_q: 0.572561\n",
            " 26478/80000: episode: 83, duration: 17.604s, episode steps: 395, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.089 [0.000, 3.000],  loss: 0.003364, mae: 0.429198, mean_q: 0.573754\n",
            " 26917/80000: episode: 84, duration: 18.719s, episode steps: 439, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.080 [0.000, 3.000],  loss: 0.003508, mae: 0.427051, mean_q: 0.570541\n",
            " 27310/80000: episode: 85, duration: 16.791s, episode steps: 393, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.372 [0.000, 3.000],  loss: 0.004077, mae: 0.431088, mean_q: 0.575772\n",
            " 27747/80000: episode: 86, duration: 20.255s, episode steps: 437, steps per second:  22, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.003615, mae: 0.428159, mean_q: 0.572052\n",
            " 28116/80000: episode: 87, duration: 16.368s, episode steps: 369, steps per second:  23, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: 0.003750, mae: 0.429733, mean_q: 0.575793\n",
            " 28419/80000: episode: 88, duration: 13.290s, episode steps: 303, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.003681, mae: 0.434793, mean_q: 0.580576\n",
            " 28631/80000: episode: 89, duration: 8.421s, episode steps: 212, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.802 [0.000, 3.000],  loss: 0.002550, mae: 0.431910, mean_q: 0.578673\n",
            " 29073/80000: episode: 90, duration: 19.862s, episode steps: 442, steps per second:  22, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.003644, mae: 0.429806, mean_q: 0.574154\n",
            " 29450/80000: episode: 91, duration: 17.248s, episode steps: 377, steps per second:  22, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.324 [0.000, 3.000],  loss: 0.003491, mae: 0.430583, mean_q: 0.575259\n",
            " 29729/80000: episode: 92, duration: 12.228s, episode steps: 279, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: 0.003376, mae: 0.435591, mean_q: 0.582653\n",
            " 30083/80000: episode: 93, duration: 16.067s, episode steps: 354, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.042 [0.000, 3.000],  loss: 0.002823, mae: 0.431027, mean_q: 0.577021\n",
            " 30335/80000: episode: 94, duration: 9.565s, episode steps: 252, steps per second:  26, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: 0.003218, mae: 0.431865, mean_q: 0.577845\n",
            " 30692/80000: episode: 95, duration: 16.368s, episode steps: 357, steps per second:  22, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.807 [0.000, 3.000],  loss: 0.003735, mae: 0.432114, mean_q: 0.577487\n",
            " 30945/80000: episode: 96, duration: 11.156s, episode steps: 253, steps per second:  23, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.814 [0.000, 3.000],  loss: 0.003629, mae: 0.439366, mean_q: 0.589040\n",
            " 31265/80000: episode: 97, duration: 14.153s, episode steps: 320, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.003991, mae: 0.440025, mean_q: 0.587890\n",
            " 31563/80000: episode: 98, duration: 13.276s, episode steps: 298, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.003388, mae: 0.438393, mean_q: 0.586547\n",
            " 31927/80000: episode: 99, duration: 16.076s, episode steps: 364, steps per second:  23, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: 0.003966, mae: 0.441942, mean_q: 0.589615\n",
            " 32258/80000: episode: 100, duration: 14.587s, episode steps: 331, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003375, mae: 0.442934, mean_q: 0.591139\n",
            " 32540/80000: episode: 101, duration: 12.661s, episode steps: 282, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.003265, mae: 0.441173, mean_q: 0.589767\n",
            " 32751/80000: episode: 102, duration: 10.050s, episode steps: 211, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.493 [0.000, 3.000],  loss: 0.003617, mae: 0.444196, mean_q: 0.594583\n",
            " 32958/80000: episode: 103, duration: 7.950s, episode steps: 207, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.493 [0.000, 3.000],  loss: 0.003390, mae: 0.443924, mean_q: 0.595497\n",
            " 33154/80000: episode: 104, duration: 9.496s, episode steps: 196, steps per second:  21, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.995 [0.000, 3.000],  loss: 0.003366, mae: 0.441961, mean_q: 0.589957\n",
            " 33523/80000: episode: 105, duration: 16.114s, episode steps: 369, steps per second:  23, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.003569, mae: 0.442333, mean_q: 0.591018\n",
            " 33827/80000: episode: 106, duration: 13.468s, episode steps: 304, steps per second:  23, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.003689, mae: 0.444522, mean_q: 0.594866\n",
            " 34231/80000: episode: 107, duration: 17.473s, episode steps: 404, steps per second:  23, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.003466, mae: 0.444471, mean_q: 0.595702\n",
            " 34458/80000: episode: 108, duration: 10.704s, episode steps: 227, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: 0.003673, mae: 0.444810, mean_q: 0.594404\n",
            " 34799/80000: episode: 109, duration: 15.266s, episode steps: 341, steps per second:  22, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.004022, mae: 0.445953, mean_q: 0.595504\n",
            " 35077/80000: episode: 110, duration: 12.697s, episode steps: 278, steps per second:  22, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.003488, mae: 0.447675, mean_q: 0.598783\n",
            " 35424/80000: episode: 111, duration: 15.238s, episode steps: 347, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.003156, mae: 0.446028, mean_q: 0.596535\n",
            " 35605/80000: episode: 112, duration: 6.876s, episode steps: 181, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.425 [0.000, 3.000],  loss: 0.004112, mae: 0.447371, mean_q: 0.599220\n",
            " 35907/80000: episode: 113, duration: 13.478s, episode steps: 302, steps per second:  22, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.003239, mae: 0.447902, mean_q: 0.598719\n",
            " 36223/80000: episode: 114, duration: 14.189s, episode steps: 316, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: 0.003275, mae: 0.443775, mean_q: 0.593431\n",
            " 36655/80000: episode: 115, duration: 18.973s, episode steps: 432, steps per second:  23, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.003444, mae: 0.442324, mean_q: 0.591186\n",
            " 37084/80000: episode: 116, duration: 19.870s, episode steps: 429, steps per second:  22, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003135, mae: 0.443004, mean_q: 0.592589\n",
            " 37461/80000: episode: 117, duration: 16.479s, episode steps: 377, steps per second:  23, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.849 [0.000, 3.000],  loss: 0.004094, mae: 0.444316, mean_q: 0.593963\n",
            " 37819/80000: episode: 118, duration: 15.548s, episode steps: 358, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.874 [0.000, 3.000],  loss: 0.003345, mae: 0.446267, mean_q: 0.596687\n",
            " 38069/80000: episode: 119, duration: 11.637s, episode steps: 250, steps per second:  21, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.003333, mae: 0.448936, mean_q: 0.600157\n",
            " 38345/80000: episode: 120, duration: 12.479s, episode steps: 276, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.957 [0.000, 3.000],  loss: 0.004013, mae: 0.449472, mean_q: 0.602022\n",
            " 38677/80000: episode: 121, duration: 14.829s, episode steps: 332, steps per second:  22, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.002823, mae: 0.450595, mean_q: 0.603459\n",
            " 38907/80000: episode: 122, duration: 9.192s, episode steps: 230, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003020, mae: 0.449866, mean_q: 0.601038\n",
            " 39193/80000: episode: 123, duration: 12.883s, episode steps: 286, steps per second:  22, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.003146, mae: 0.450232, mean_q: 0.601026\n",
            " 39547/80000: episode: 124, duration: 15.675s, episode steps: 354, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.003760, mae: 0.450178, mean_q: 0.600668\n",
            " 39883/80000: episode: 125, duration: 15.030s, episode steps: 336, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.190 [0.000, 3.000],  loss: 0.003218, mae: 0.447592, mean_q: 0.599255\n",
            " 40116/80000: episode: 126, duration: 11.067s, episode steps: 233, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.003696, mae: 0.445310, mean_q: 0.594284\n",
            " 40403/80000: episode: 127, duration: 12.871s, episode steps: 287, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.160 [0.000, 3.000],  loss: 0.003322, mae: 0.447310, mean_q: 0.598700\n",
            " 40599/80000: episode: 128, duration: 7.650s, episode steps: 196, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.005 [0.000, 3.000],  loss: 0.003515, mae: 0.446382, mean_q: 0.598445\n",
            " 40845/80000: episode: 129, duration: 11.535s, episode steps: 246, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.110 [0.000, 3.000],  loss: 0.003454, mae: 0.449260, mean_q: 0.601076\n",
            " 41152/80000: episode: 130, duration: 13.887s, episode steps: 307, steps per second:  22, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.003115, mae: 0.445010, mean_q: 0.593845\n",
            " 41432/80000: episode: 131, duration: 12.766s, episode steps: 280, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.003661, mae: 0.443809, mean_q: 0.591018\n",
            " 41693/80000: episode: 132, duration: 11.943s, episode steps: 261, steps per second:  22, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.002978, mae: 0.441882, mean_q: 0.591042\n",
            " 41866/80000: episode: 133, duration: 6.593s, episode steps: 173, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.006 [0.000, 3.000],  loss: 0.003059, mae: 0.440576, mean_q: 0.590142\n",
            " 42103/80000: episode: 134, duration: 11.030s, episode steps: 237, steps per second:  21, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.002906, mae: 0.439058, mean_q: 0.587717\n",
            " 42353/80000: episode: 135, duration: 11.528s, episode steps: 250, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.980 [0.000, 3.000],  loss: 0.002767, mae: 0.438751, mean_q: 0.586228\n",
            " 42579/80000: episode: 136, duration: 8.879s, episode steps: 226, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.960 [0.000, 3.000],  loss: 0.003179, mae: 0.436950, mean_q: 0.583137\n",
            " 43048/80000: episode: 137, duration: 22.293s, episode steps: 469, steps per second:  21, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.772 [0.000, 3.000],  loss: 0.003271, mae: 0.439884, mean_q: 0.587947\n",
            " 43297/80000: episode: 138, duration: 11.662s, episode steps: 249, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002437, mae: 0.437892, mean_q: 0.586016\n",
            " 43821/80000: episode: 139, duration: 22.059s, episode steps: 524, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.939 [0.000, 3.000],  loss: 0.003151, mae: 0.435716, mean_q: 0.582038\n",
            " 44138/80000: episode: 140, duration: 13.998s, episode steps: 317, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.284 [0.000, 3.000],  loss: 0.003055, mae: 0.435086, mean_q: 0.582670\n",
            " 44572/80000: episode: 141, duration: 19.620s, episode steps: 434, steps per second:  22, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.825 [0.000, 3.000],  loss: 0.003127, mae: 0.432480, mean_q: 0.578129\n",
            " 44838/80000: episode: 142, duration: 11.186s, episode steps: 266, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002846, mae: 0.429778, mean_q: 0.574050\n",
            " 45258/80000: episode: 143, duration: 19.836s, episode steps: 420, steps per second:  21, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.003564, mae: 0.428860, mean_q: 0.571881\n",
            " 45503/80000: episode: 144, duration: 9.569s, episode steps: 245, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.967 [0.000, 3.000],  loss: 0.003998, mae: 0.431543, mean_q: 0.576764\n",
            " 45753/80000: episode: 145, duration: 11.735s, episode steps: 250, steps per second:  21, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.120 [0.000, 3.000],  loss: 0.003409, mae: 0.433765, mean_q: 0.580153\n",
            " 46069/80000: episode: 146, duration: 14.317s, episode steps: 316, steps per second:  22, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.003434, mae: 0.432154, mean_q: 0.577511\n",
            " 46460/80000: episode: 147, duration: 17.094s, episode steps: 391, steps per second:  23, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.003028, mae: 0.431938, mean_q: 0.577393\n",
            " 46864/80000: episode: 148, duration: 18.776s, episode steps: 404, steps per second:  22, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: 0.003257, mae: 0.432638, mean_q: 0.578264\n",
            " 47249/80000: episode: 149, duration: 17.842s, episode steps: 385, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.003155, mae: 0.429882, mean_q: 0.574673\n",
            " 47490/80000: episode: 150, duration: 9.676s, episode steps: 241, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.954 [0.000, 3.000],  loss: 0.003528, mae: 0.430951, mean_q: 0.574768\n",
            " 47885/80000: episode: 151, duration: 19.315s, episode steps: 395, steps per second:  20, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.301 [0.000, 3.000],  loss: 0.002955, mae: 0.430723, mean_q: 0.576105\n",
            " 48114/80000: episode: 152, duration: 8.847s, episode steps: 229, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.003151, mae: 0.429721, mean_q: 0.574442\n",
            " 48296/80000: episode: 153, duration: 9.178s, episode steps: 182, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.110 [0.000, 3.000],  loss: 0.003511, mae: 0.431332, mean_q: 0.574842\n",
            " 48516/80000: episode: 154, duration: 10.410s, episode steps: 220, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.123 [0.000, 3.000],  loss: 0.003243, mae: 0.433740, mean_q: 0.579421\n",
            " 48980/80000: episode: 155, duration: 20.618s, episode steps: 464, steps per second:  23, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.003188, mae: 0.431746, mean_q: 0.578499\n",
            " 49343/80000: episode: 156, duration: 16.099s, episode steps: 363, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.003301, mae: 0.431293, mean_q: 0.575971\n",
            " 49641/80000: episode: 157, duration: 13.483s, episode steps: 298, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.322 [0.000, 3.000],  loss: 0.003520, mae: 0.435405, mean_q: 0.583079\n",
            " 49923/80000: episode: 158, duration: 12.869s, episode steps: 282, steps per second:  22, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002882, mae: 0.435770, mean_q: 0.582762\n",
            " 50362/80000: episode: 159, duration: 18.803s, episode steps: 439, steps per second:  23, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.003407, mae: 0.433166, mean_q: 0.578162\n",
            " 50635/80000: episode: 160, duration: 12.538s, episode steps: 273, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.923 [0.000, 3.000],  loss: 0.002456, mae: 0.431302, mean_q: 0.578138\n",
            " 50927/80000: episode: 161, duration: 13.527s, episode steps: 292, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.890 [0.000, 3.000],  loss: 0.002307, mae: 0.429176, mean_q: 0.573772\n",
            " 51306/80000: episode: 162, duration: 16.856s, episode steps: 379, steps per second:  22, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.172 [0.000, 3.000],  loss: 0.003195, mae: 0.426912, mean_q: 0.570337\n",
            " 51662/80000: episode: 163, duration: 15.677s, episode steps: 356, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.798 [0.000, 3.000],  loss: 0.003319, mae: 0.425006, mean_q: 0.566979\n",
            " 51978/80000: episode: 164, duration: 14.107s, episode steps: 316, steps per second:  22, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.003194, mae: 0.426027, mean_q: 0.569038\n",
            " 52410/80000: episode: 165, duration: 19.665s, episode steps: 432, steps per second:  22, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: 0.002535, mae: 0.420512, mean_q: 0.562242\n",
            " 52732/80000: episode: 166, duration: 14.490s, episode steps: 322, steps per second:  22, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.003204, mae: 0.420910, mean_q: 0.562373\n",
            " 53024/80000: episode: 167, duration: 12.416s, episode steps: 292, steps per second:  24, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.058 [0.000, 3.000],  loss: 0.003269, mae: 0.419777, mean_q: 0.561613\n",
            " 53388/80000: episode: 168, duration: 17.096s, episode steps: 364, steps per second:  21, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.003179, mae: 0.418678, mean_q: 0.559414\n",
            " 53665/80000: episode: 169, duration: 11.616s, episode steps: 277, steps per second:  24, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.002796, mae: 0.418942, mean_q: 0.560777\n",
            " 54030/80000: episode: 170, duration: 16.843s, episode steps: 365, steps per second:  22, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.003070, mae: 0.417979, mean_q: 0.559326\n",
            " 54248/80000: episode: 171, duration: 9.757s, episode steps: 218, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.390 [0.000, 3.000],  loss: 0.003793, mae: 0.422960, mean_q: 0.564671\n",
            " 54482/80000: episode: 172, duration: 10.999s, episode steps: 234, steps per second:  21, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.197 [0.000, 3.000],  loss: 0.002682, mae: 0.417571, mean_q: 0.557828\n",
            " 54681/80000: episode: 173, duration: 8.334s, episode steps: 199, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: 0.002820, mae: 0.417100, mean_q: 0.556969\n",
            " 55041/80000: episode: 174, duration: 16.705s, episode steps: 360, steps per second:  22, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003410, mae: 0.419403, mean_q: 0.560572\n",
            " 55499/80000: episode: 175, duration: 20.149s, episode steps: 458, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.003427, mae: 0.420310, mean_q: 0.562187\n",
            " 55740/80000: episode: 176, duration: 11.468s, episode steps: 241, steps per second:  21, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.002899, mae: 0.420085, mean_q: 0.561869\n",
            " 55966/80000: episode: 177, duration: 9.157s, episode steps: 226, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.208 [0.000, 3.000],  loss: 0.003455, mae: 0.419930, mean_q: 0.561701\n",
            " 56237/80000: episode: 178, duration: 12.213s, episode steps: 271, steps per second:  22, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.002587, mae: 0.420116, mean_q: 0.561802\n",
            " 56426/80000: episode: 179, duration: 9.371s, episode steps: 189, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.376 [0.000, 3.000],  loss: 0.003064, mae: 0.418060, mean_q: 0.559092\n",
            " 56758/80000: episode: 180, duration: 14.817s, episode steps: 332, steps per second:  22, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.003229, mae: 0.419642, mean_q: 0.562711\n",
            " 56992/80000: episode: 181, duration: 10.843s, episode steps: 234, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.782 [0.000, 3.000],  loss: 0.002299, mae: 0.417348, mean_q: 0.557290\n",
            " 57283/80000: episode: 182, duration: 12.111s, episode steps: 291, steps per second:  24, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.003030, mae: 0.415562, mean_q: 0.554856\n",
            " 57687/80000: episode: 183, duration: 18.848s, episode steps: 404, steps per second:  21, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.003097, mae: 0.417340, mean_q: 0.558268\n",
            " 58105/80000: episode: 184, duration: 17.935s, episode steps: 418, steps per second:  23, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.002617, mae: 0.416824, mean_q: 0.558136\n",
            " 58458/80000: episode: 185, duration: 15.536s, episode steps: 353, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.856 [0.000, 3.000],  loss: 0.002734, mae: 0.416782, mean_q: 0.558309\n",
            " 58829/80000: episode: 186, duration: 16.237s, episode steps: 371, steps per second:  23, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002817, mae: 0.414830, mean_q: 0.554012\n",
            " 59152/80000: episode: 187, duration: 14.539s, episode steps: 323, steps per second:  22, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.991 [0.000, 3.000],  loss: 0.002735, mae: 0.412408, mean_q: 0.551138\n",
            " 59473/80000: episode: 188, duration: 14.498s, episode steps: 321, steps per second:  22, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.184 [0.000, 3.000],  loss: 0.002731, mae: 0.410669, mean_q: 0.549198\n",
            " 59743/80000: episode: 189, duration: 12.561s, episode steps: 270, steps per second:  21, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.002487, mae: 0.410320, mean_q: 0.549057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import GreedyQPolicy\n",
        "from gym.wrappers import AtariPreprocessing\n",
        "\n",
        "class CompatibleAtariPreprocessing(gym.Wrapper):\n",
        "    def __init__(self, env, **kwargs):\n",
        "        super().__init__(env)\n",
        "        self.env = AtariPreprocessing(env, **kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info, truncated = self.env.step(action)\n",
        "        # Ensure info is a dictionary\n",
        "        if not isinstance(info, dict):\n",
        "            info = {}  # or handle appropriately if there's meaningful data in 'info'\n",
        "        return observation, reward, done, info  # return only the first four elements\n",
        "\n",
        "# Environment setup with custom preprocessing wrapper\n",
        "env = gym.make('Breakout-v4')\n",
        "env = CompatibleAtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=1, scale_obs=True, new_step_api=True)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "\n",
        "# Adjusting the model to the preprocessed observation space\n",
        "input_shape = (1,) + env.observation_space.shape\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "model.add(Dense(24, activation='relu'))\n",
        "model.add(Dense(24, activation='relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "\n",
        "# Configure the agent\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = GreedyQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, policy=policy)\n",
        "dqn.compile(optimizer=Adam())\n",
        "\n",
        "# Train the agent\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "\n",
        "# Save the final policy network after training\n",
        "dqn.save_weights('policy_final.h5', overwrite=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMARJIqKBsN-",
        "outputId": "5ab140d8-be29-4e75-fc8b-2021dbdb1b78"
      },
      "execution_count": 24,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'tuple'>\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:226: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
            "  logger.warn(\"Casting input x to numpy array.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:167: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space with exception: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space with exception: {e}\")\n",
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1424/50000: episode: 1, duration: 21.242s, episode steps: 1424, steps per second:  67, episode reward:  4.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.575 [0.000, 2.000],  loss: 0.001543, mean_q: 0.068403\n",
            "  1859/50000: episode: 2, duration: 16.622s, episode steps: 435, steps per second:  26, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 0.841 [0.000, 2.000],  loss: 0.002399, mean_q: 0.006847\n",
            "  2142/50000: episode: 3, duration: 11.404s, episode steps: 283, steps per second:  25, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.721 [0.000, 2.000],  loss: 0.002489, mean_q: -0.002551\n",
            "  2571/50000: episode: 4, duration: 16.421s, episode steps: 429, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.303 [0.000, 3.000],  loss: 0.002557, mean_q: -0.000348\n",
            "  2813/50000: episode: 5, duration: 10.061s, episode steps: 242, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.905 [0.000, 3.000],  loss: 0.002373, mean_q: -0.001450\n",
            " 10276/50000: episode: 6, duration: 289.849s, episode steps: 7463, steps per second:  26, episode reward:  2.000, mean reward:  0.000 [ 0.000,  1.000], mean action: 2.897 [0.000, 3.000],  loss: 0.001070, mean_q: 0.012024\n",
            " 10622/50000: episode: 7, duration: 14.469s, episode steps: 346, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.647 [0.000, 2.000],  loss: 0.000676, mean_q: 0.021943\n",
            " 10880/50000: episode: 8, duration: 10.884s, episode steps: 258, steps per second:  24, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 0.624 [0.000, 2.000],  loss: 0.000667, mean_q: 0.018687\n",
            " 11391/50000: episode: 9, duration: 20.268s, episode steps: 511, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.329 [0.000, 2.000],  loss: 0.000703, mean_q: 0.019040\n",
            " 11565/50000: episode: 10, duration: 6.877s, episode steps: 174, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.718 [0.000, 1.000],  loss: 0.000809, mean_q: 0.022366\n",
            " 11983/50000: episode: 11, duration: 16.227s, episode steps: 418, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.591 [0.000, 2.000],  loss: 0.000711, mean_q: 0.018251\n",
            " 12367/50000: episode: 12, duration: 15.323s, episode steps: 384, steps per second:  25, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 0.568 [0.000, 2.000],  loss: 0.000935, mean_q: 0.020289\n",
            " 12919/50000: episode: 13, duration: 22.928s, episode steps: 552, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.902 [0.000, 3.000],  loss: 0.000709, mean_q: 0.017189\n",
            " 13215/50000: episode: 14, duration: 10.056s, episode steps: 296, steps per second:  29, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.001478, mean_q: 0.020631\n",
            " 13847/50000: episode: 15, duration: 25.038s, episode steps: 632, steps per second:  25, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.491 [0.000, 2.000],  loss: 0.000817, mean_q: 0.017871\n",
            " 14007/50000: episode: 16, duration: 6.390s, episode steps: 160, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.001367, mean_q: 0.022054\n",
            " 14292/50000: episode: 17, duration: 10.546s, episode steps: 285, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.765 [0.000, 2.000],  loss: 0.000822, mean_q: 0.017576\n",
            " 14898/50000: episode: 18, duration: 24.324s, episode steps: 606, steps per second:  25, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 0.911 [0.000, 3.000],  loss: 0.000621, mean_q: 0.015799\n",
            " 15105/50000: episode: 19, duration: 7.046s, episode steps: 207, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.715 [0.000, 2.000],  loss: 0.000831, mean_q: 0.016423\n",
            " 15600/50000: episode: 20, duration: 20.720s, episode steps: 495, steps per second:  24, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 0.671 [0.000, 2.000],  loss: 0.001231, mean_q: 0.019691\n",
            " 15838/50000: episode: 21, duration: 8.007s, episode steps: 238, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 0.001049, mean_q: 0.018759\n",
            " 16017/50000: episode: 22, duration: 8.075s, episode steps: 179, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.000786, mean_q: 0.017031\n",
            " 16295/50000: episode: 23, duration: 10.780s, episode steps: 278, steps per second:  26, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.817 [0.000, 2.000],  loss: 0.001124, mean_q: 0.017766\n",
            " 16595/50000: episode: 24, duration: 10.567s, episode steps: 300, steps per second:  28, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.001249, mean_q: 0.020058\n",
            " 17201/50000: episode: 25, duration: 24.335s, episode steps: 606, steps per second:  25, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.634 [0.000, 2.000],  loss: 0.001006, mean_q: 0.017816\n",
            " 17468/50000: episode: 26, duration: 11.104s, episode steps: 267, steps per second:  24, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.876 [0.000, 2.000],  loss: 0.001112, mean_q: 0.018322\n",
            " 17678/50000: episode: 27, duration: 7.007s, episode steps: 210, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.001116, mean_q: 0.018176\n",
            " 18180/50000: episode: 28, duration: 20.394s, episode steps: 502, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.610 [0.000, 2.000],  loss: 0.001336, mean_q: 0.018455\n",
            " 18615/50000: episode: 29, duration: 17.266s, episode steps: 435, steps per second:  25, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.001470, mean_q: 0.019493\n",
            " 19008/50000: episode: 30, duration: 15.191s, episode steps: 393, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.001587, mean_q: 0.019049\n",
            " 19225/50000: episode: 31, duration: 7.413s, episode steps: 217, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 1.000],  loss: 0.001295, mean_q: 0.018060\n",
            " 19598/50000: episode: 32, duration: 14.661s, episode steps: 373, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 2.000],  loss: 0.001297, mean_q: 0.018976\n",
            " 19941/50000: episode: 33, duration: 13.668s, episode steps: 343, steps per second:  25, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.668 [0.000, 2.000],  loss: 0.001275, mean_q: 0.018299\n",
            " 20488/50000: episode: 34, duration: 22.697s, episode steps: 547, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.137 [0.000, 3.000],  loss: 0.001314, mean_q: 0.023375\n",
            " 20841/50000: episode: 35, duration: 13.983s, episode steps: 353, steps per second:  25, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.000842, mean_q: 0.021471\n",
            " 21251/50000: episode: 36, duration: 15.893s, episode steps: 410, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.539 [0.000, 2.000],  loss: 0.000763, mean_q: 0.021968\n",
            " 21624/50000: episode: 37, duration: 14.572s, episode steps: 373, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.735 [0.000, 2.000],  loss: 0.001256, mean_q: 0.023380\n",
            " 22067/50000: episode: 38, duration: 17.023s, episode steps: 443, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.142 [0.000, 2.000],  loss: 0.001271, mean_q: 0.024551\n",
            " 22440/50000: episode: 39, duration: 14.750s, episode steps: 373, steps per second:  25, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.001423, mean_q: 0.025022\n",
            " 22609/50000: episode: 40, duration: 5.776s, episode steps: 169, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.929 [0.000, 1.000],  loss: 0.001659, mean_q: 0.026417\n",
            " 22919/50000: episode: 41, duration: 12.596s, episode steps: 310, steps per second:  25, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: 0.001209, mean_q: 0.023125\n",
            " 23245/50000: episode: 42, duration: 13.185s, episode steps: 326, steps per second:  25, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.877 [0.000, 3.000],  loss: 0.000960, mean_q: 0.022984\n",
            " 23567/50000: episode: 43, duration: 13.175s, episode steps: 322, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.003 [0.000, 3.000],  loss: 0.001263, mean_q: 0.024859\n",
            " 24041/50000: episode: 44, duration: 18.118s, episode steps: 474, steps per second:  26, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.001678, mean_q: 0.025161\n",
            " 24248/50000: episode: 45, duration: 9.063s, episode steps: 207, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.001506, mean_q: 0.022488\n",
            " 24584/50000: episode: 46, duration: 13.492s, episode steps: 336, steps per second:  25, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.324 [0.000, 3.000],  loss: 0.000979, mean_q: 0.022943\n",
            " 24932/50000: episode: 47, duration: 13.791s, episode steps: 348, steps per second:  25, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.264 [0.000, 3.000],  loss: 0.001256, mean_q: 0.023915\n",
            " 25402/50000: episode: 48, duration: 18.259s, episode steps: 470, steps per second:  26, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.132 [0.000, 3.000],  loss: 0.001300, mean_q: 0.023862\n",
            " 25714/50000: episode: 49, duration: 12.632s, episode steps: 312, steps per second:  25, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.001202, mean_q: 0.023160\n",
            " 25898/50000: episode: 50, duration: 6.205s, episode steps: 184, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.000765, mean_q: 0.021430\n",
            " 26403/50000: episode: 51, duration: 20.682s, episode steps: 505, steps per second:  24, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.001821, mean_q: 0.027908\n",
            " 26809/50000: episode: 52, duration: 16.503s, episode steps: 406, steps per second:  25, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.001462, mean_q: 0.025160\n",
            " 27080/50000: episode: 53, duration: 9.197s, episode steps: 271, steps per second:  29, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.284 [1.000, 3.000],  loss: 0.001095, mean_q: 0.022794\n",
            " 27384/50000: episode: 54, duration: 12.369s, episode steps: 304, steps per second:  25, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.839 [0.000, 2.000],  loss: 0.001028, mean_q: 0.022547\n",
            " 27595/50000: episode: 55, duration: 9.339s, episode steps: 211, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.327 [0.000, 2.000],  loss: 0.001406, mean_q: 0.024836\n",
            " 28136/50000: episode: 56, duration: 20.661s, episode steps: 541, steps per second:  26, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.610 [0.000, 2.000],  loss: 0.001586, mean_q: 0.024115\n",
            " 28688/50000: episode: 57, duration: 23.725s, episode steps: 552, steps per second:  23, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 0.824 [0.000, 3.000],  loss: 0.001104, mean_q: 0.022537\n",
            " 28915/50000: episode: 58, duration: 8.604s, episode steps: 227, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.537 [0.000, 2.000],  loss: 0.001650, mean_q: 0.024099\n",
            " 29205/50000: episode: 59, duration: 11.983s, episode steps: 290, steps per second:  24, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.001453, mean_q: 0.024836\n",
            " 29367/50000: episode: 60, duration: 7.619s, episode steps: 162, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [1.000, 3.000],  loss: 0.002016, mean_q: 0.024939\n",
            " 29654/50000: episode: 61, duration: 10.366s, episode steps: 287, steps per second:  28, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.857 [0.000, 3.000],  loss: 0.001795, mean_q: 0.025628\n",
            " 30267/50000: episode: 62, duration: 24.549s, episode steps: 613, steps per second:  25, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.767 [0.000, 2.000],  loss: 0.001832, mean_q: 0.028546\n",
            " 30603/50000: episode: 63, duration: 13.608s, episode steps: 336, steps per second:  25, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.001348, mean_q: 0.034397\n",
            " 31012/50000: episode: 64, duration: 16.348s, episode steps: 409, steps per second:  25, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.001414, mean_q: 0.034227\n",
            " 31301/50000: episode: 65, duration: 12.203s, episode steps: 289, steps per second:  24, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.001404, mean_q: 0.034254\n",
            " 31605/50000: episode: 66, duration: 12.408s, episode steps: 304, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.829 [0.000, 3.000],  loss: 0.001902, mean_q: 0.035923\n",
            " 31846/50000: episode: 67, duration: 9.055s, episode steps: 241, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.001492, mean_q: 0.035372\n",
            " 32109/50000: episode: 68, duration: 9.492s, episode steps: 263, steps per second:  28, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 0.954 [0.000, 3.000],  loss: 0.001955, mean_q: 0.034330\n",
            " 32328/50000: episode: 69, duration: 9.143s, episode steps: 219, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.001290, mean_q: 0.035463\n",
            " 32529/50000: episode: 70, duration: 7.332s, episode steps: 201, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.001323, mean_q: 0.034498\n",
            " 32860/50000: episode: 71, duration: 12.549s, episode steps: 331, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.381 [0.000, 2.000],  loss: 0.001087, mean_q: 0.033160\n",
            " 33424/50000: episode: 72, duration: 22.384s, episode steps: 564, steps per second:  25, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.011 [0.000, 3.000],  loss: 0.001329, mean_q: 0.034002\n",
            " 33687/50000: episode: 73, duration: 10.552s, episode steps: 263, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 0.001482, mean_q: 0.034139\n",
            " 33895/50000: episode: 74, duration: 7.109s, episode steps: 208, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.736 [0.000, 1.000],  loss: 0.001200, mean_q: 0.035035\n",
            " 34226/50000: episode: 75, duration: 12.977s, episode steps: 331, steps per second:  26, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.927 [0.000, 2.000],  loss: 0.001651, mean_q: 0.034932\n",
            " 34396/50000: episode: 76, duration: 7.544s, episode steps: 170, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.001470, mean_q: 0.033551\n",
            " 34799/50000: episode: 77, duration: 15.449s, episode steps: 403, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.001819, mean_q: 0.035429\n",
            " 35083/50000: episode: 78, duration: 11.201s, episode steps: 284, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.001811, mean_q: 0.035802\n",
            " 35526/50000: episode: 79, duration: 16.784s, episode steps: 443, steps per second:  26, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.001482, mean_q: 0.034314\n",
            " 35685/50000: episode: 80, duration: 5.651s, episode steps: 159, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.002448, mean_q: 0.038779\n",
            " 35972/50000: episode: 81, duration: 11.292s, episode steps: 287, steps per second:  25, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.129 [0.000, 3.000],  loss: 0.001632, mean_q: 0.034725\n",
            " 36135/50000: episode: 82, duration: 7.177s, episode steps: 163, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.926 [0.000, 1.000],  loss: 0.001914, mean_q: 0.035851\n",
            " 36437/50000: episode: 83, duration: 11.899s, episode steps: 302, steps per second:  25, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.947 [0.000, 2.000],  loss: 0.001447, mean_q: 0.033825\n",
            " 36806/50000: episode: 84, duration: 14.414s, episode steps: 369, steps per second:  26, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 0.973 [0.000, 3.000],  loss: 0.001482, mean_q: 0.034067\n",
            " 37283/50000: episode: 85, duration: 17.804s, episode steps: 477, steps per second:  27, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.818 [0.000, 2.000],  loss: 0.001408, mean_q: 0.033866\n",
            " 37711/50000: episode: 86, duration: 16.503s, episode steps: 428, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.001533, mean_q: 0.034547\n",
            " 38062/50000: episode: 87, duration: 13.590s, episode steps: 351, steps per second:  26, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.142 [0.000, 3.000],  loss: 0.001069, mean_q: 0.033270\n",
            " 38252/50000: episode: 88, duration: 8.299s, episode steps: 190, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.001973, mean_q: 0.035093\n",
            " 38662/50000: episode: 89, duration: 15.955s, episode steps: 410, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.001448, mean_q: 0.034074\n",
            " 38915/50000: episode: 90, duration: 10.597s, episode steps: 253, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.711 [0.000, 2.000],  loss: 0.001602, mean_q: 0.034986\n",
            " 39252/50000: episode: 91, duration: 13.401s, episode steps: 337, steps per second:  25, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.380 [0.000, 3.000],  loss: 0.001668, mean_q: 0.034778\n",
            " 39586/50000: episode: 92, duration: 13.260s, episode steps: 334, steps per second:  25, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.001961, mean_q: 0.036403\n",
            " 39756/50000: episode: 93, duration: 5.928s, episode steps: 170, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.582 [0.000, 1.000],  loss: 0.002199, mean_q: 0.036962\n",
            " 40112/50000: episode: 94, duration: 13.998s, episode steps: 356, steps per second:  25, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.834 [0.000, 2.000],  loss: 0.001494, mean_q: 0.035380\n",
            " 40506/50000: episode: 95, duration: 15.214s, episode steps: 394, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.779 [0.000, 2.000],  loss: 0.001469, mean_q: 0.041909\n",
            " 40875/50000: episode: 96, duration: 14.420s, episode steps: 369, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.851 [0.000, 2.000],  loss: 0.001735, mean_q: 0.042963\n",
            " 41205/50000: episode: 97, duration: 12.863s, episode steps: 330, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.912 [0.000, 3.000],  loss: 0.001988, mean_q: 0.044112\n",
            " 41641/50000: episode: 98, duration: 18.215s, episode steps: 436, steps per second:  24, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.991 [0.000, 2.000],  loss: 0.001897, mean_q: 0.044251\n",
            " 42082/50000: episode: 99, duration: 17.284s, episode steps: 441, steps per second:  26, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.372 [0.000, 3.000],  loss: 0.001665, mean_q: 0.043318\n",
            " 42503/50000: episode: 100, duration: 16.371s, episode steps: 421, steps per second:  26, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.001448, mean_q: 0.041483\n",
            " 43040/50000: episode: 101, duration: 22.010s, episode steps: 537, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.001917, mean_q: 0.043051\n",
            " 43427/50000: episode: 102, duration: 14.927s, episode steps: 387, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.269 [0.000, 3.000],  loss: 0.001614, mean_q: 0.043175\n",
            " 43734/50000: episode: 103, duration: 12.268s, episode steps: 307, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 2.000],  loss: 0.001375, mean_q: 0.041415\n",
            " 44231/50000: episode: 104, duration: 19.325s, episode steps: 497, steps per second:  26, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.001510, mean_q: 0.042106\n",
            " 44640/50000: episode: 105, duration: 15.653s, episode steps: 409, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: 0.001681, mean_q: 0.043054\n",
            " 45184/50000: episode: 106, duration: 21.885s, episode steps: 544, steps per second:  25, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.001777, mean_q: 0.044467\n",
            " 45546/50000: episode: 107, duration: 14.332s, episode steps: 362, steps per second:  25, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.298 [0.000, 3.000],  loss: 0.002284, mean_q: 0.044719\n",
            " 45846/50000: episode: 108, duration: 11.986s, episode steps: 300, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.001456, mean_q: 0.041754\n",
            " 46152/50000: episode: 109, duration: 12.288s, episode steps: 306, steps per second:  25, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.001584, mean_q: 0.042128\n",
            " 46469/50000: episode: 110, duration: 12.727s, episode steps: 317, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.675 [0.000, 2.000],  loss: 0.001528, mean_q: 0.042261\n",
            " 46871/50000: episode: 111, duration: 16.205s, episode steps: 402, steps per second:  25, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.913 [0.000, 2.000],  loss: 0.001130, mean_q: 0.041015\n",
            " 47237/50000: episode: 112, duration: 14.372s, episode steps: 366, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.001920, mean_q: 0.043771\n",
            " 47678/50000: episode: 113, duration: 17.241s, episode steps: 441, steps per second:  26, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.098 [0.000, 3.000],  loss: 0.001559, mean_q: 0.042199\n",
            " 48127/50000: episode: 114, duration: 18.513s, episode steps: 449, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.001739, mean_q: 0.042991\n",
            " 48601/50000: episode: 115, duration: 19.115s, episode steps: 474, steps per second:  25, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.774 [0.000, 2.000],  loss: 0.001681, mean_q: 0.042639\n",
            " 49173/50000: episode: 116, duration: 23.545s, episode steps: 572, steps per second:  24, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.241 [0.000, 3.000],  loss: 0.001339, mean_q: 0.040934\n",
            " 49556/50000: episode: 117, duration: 15.207s, episode steps: 383, steps per second:  25, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.002487, mean_q: 0.043896\n",
            " 49768/50000: episode: 118, duration: 7.646s, episode steps: 212, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.321 [0.000, 3.000],  loss: 0.001475, mean_q: 0.042037\n",
            "done, took 1935.301 seconds\n"
          ]
        }
      ]
    }
  ]
}